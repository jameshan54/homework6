---
title: "homework6"
author: "James(Changhwan) Han (3923257)"
date: "5/20/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, warning = FALSE}
# install.packages("glmnet")
# install.packages("janitor")
# install.packages("ranger")
library(tidymodels)
library(tidyverse)
library(rlang)
library(knitr)
library(discrim)
library(klaR)
library(glmnet)
library("janitor")
library(corrplot)
library(rpart.plot)
library(ranger)
library(vip)
tidymodels_prefer()
```

```{r}
pokemon <- read_csv("Pokemon.csv")
head(pokemon)
```
# Exercise 1
```{r}
pokemon <- pokemon %>% 
  clean_names()

df <- pokemon %>%
  filter(type_1 %in% c("Bug", "Fire", "Grass", "Normal", "Water", "Psychic"))

df$type_1 <- factor(df$type_1)
df$legendary <- factor(df$legendary)

set.seed(1014)

df_split <- initial_split(df, prop = 0.70,
                          strata = type_1)
df_train <- training(df_split)
df_test <- testing(df_split)

dim(df_train)
dim(df_test)

df_folds <- vfold_cv(df_train, v = 5, strata = type_1)
df_folds

df_recipe <- recipe(type_1 ~ legendary + generation + sp_atk + attack + speed + defense + hp + sp_def,
                    data = df_train) %>%
  step_dummy(c(legendary, generation)) %>%
  step_normalize(all_predictors())

head(pokemon)
```

# Exercise 2
```{r}
df_train %>%
  select(is.numeric) %>%
  cor() %>%
  corrplot(type = 'lower', diag = FALSE,
           method = 'color')
```
1. How to handle the continuous variables for this plot?
  I took all the continuous variables into account by using select(is.numeric) but we won't consider number and generation here.
2. What relationships do you notice?
  Total has very strong positive relationships with other variables.

# Exercise 3

```{r}
tree_spec <- decision_tree() %>%
  set_engine("rpart")

class_tree_spec <- tree_spec %>%
  set_mode("classification")

class_tree_wf <- workflow() %>%
  add_model(class_tree_spec %>% set_args(cost_complexity = tune())) %>%
  add_recipe(df_recipe)

param_grid <- grid_regular(cost_complexity(range = c(-3, -1)), levels = 10)

tune_res <- tune_grid(
  class_tree_wf,
  resamples = df_folds,
  grid = param_grid,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res)
```
1. Does a single decision tree perform better with a smaller or larger complexity penalty?
  A single decision tree perform better with a smaller complexity penalty. This is because 'roc_auc' increases when complexity when complexity penalty decreases. 
  
# Exercise 4
```{r}
collect_metrics(tune_res)
arrange(tune_res)
best_complexity <- select_best(tune_res)
best_complexity
```
1. What is the roc_auc of your best-performing pruned decision tree on the folds?
  My best-performing pruned decision tree on the folds is 0.679.
  
# Exercise 5-1
```{r}
class_tree_final <- finalize_workflow(class_tree_wf, best_complexity)

class_tree_final_fit <- fit(class_tree_final, data = df_train)

class_tree_final_fit %>%
  extract_fit_engine() %>%
  rpart.plot()
```
# Exercise 5-2
```{r}
class_forest_spec <- rand_forest() %>%
  set_engine("ranger", importance = "impurity") %>%
  set_mode("classification")

param_grid2 <- grid_regular(mtry(range = c(1, 8)), trees(range = c(1,8)), min_n(range = c(1,8)),  levels = 8)

forest_workflow <- workflow() %>%
  add_model(class_forest_spec %>% set_args(mtry = tune(), trees = tune(), min_n = tune())) %>%
  add_recipe(df_recipe)
```
1. mtry shouldn't be smaller than 1 or larger than 8. Explain why not
  mtry should fall between 1 and 8 because there are 8 variables, hence, the maximum for mtry is 8.
2. What type of model would mtry=8 represent?
  mtry=8 represents the creation of decision tree and having a look at 8 of the features.
  
# Exercise 6
```{r}
tune_res_forest <- tune_grid(
  forest_workflow,
  resamples = df_folds,
  grid = param_grid2,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_forest)
```
1. What values of the hyperparameters seem to yield the best performance?

# Exercise 7
```{r}
collect_metrics(tune_res_forest)
arrange(tune_res_forest)
best_complexity2 <- select_best(tune_res_forest)
best_complexity2
```
1. What is the roc_auc of your best-performing random forest model on the folds?

# Exercise 8
```{r}
class_tree_final_fit %>%
  pull_workflow_fit() %>%
  vip()
```
1. Which variables were most/least useful? Are these results what you expected? 

# Exercise 9 
```{r}
boost_spec <- boost_tree(trees = c(10,2000), tree_depth = 4) %>%
  set_engine("xgboost") %>%
  set_mode("classification")

param_grid_boost <- grid_regular(trees(range = c(10, 2000)),  levels = 10)

boost_workflow <- workflow() %>%
  add_model(boost_spec %>% set_args(trees = tune())) %>%
  add_recipe(df_recipe)

tune_res_boost <- tune_grid(
  boost_workflow,
  resamples = df_folds,
  grid = param_grid_boost,
  metrics = metric_set(roc_auc)
)

autoplot(tune_res_boost)

collect_metrics(tune_res_boost)
arrange(tune_res_boost)
best_complexity3 <- select_best(tune_res_boost)
best_complexity3
```
1. What is the roc_auc of your best-performing boosted tree model on the folds?
  the roc_auc of my best-performing boosted tree model on the folds is 0.718.
  
# Exercise 10
```{r}
df <- data.frame(best_performing = c(0.6683, 0.7219, 0.6944),
                 models <- c("pruned tree model", "random forest model", "boosted tree model"))

head(df)

#fit it to the testing set
best_complexity <- select_best(tune_res)

class_tree_final <- finalize_workflow(forest_workflow, best_complexity2)

class_tree_final_fit <- fit(class_tree_final, data = df_test)

pred_result <- augment(class_tree_final_fit, new_data = df_test)
auc <- roc_auc(data = pred_result, truth = type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic ), estimator = "macro_weighted")
auc

augment(class_tree_final_fit, new_data = df_test) %>%
  roc_curve(type_1, estimate = .pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic) %>%
  autoplot()

augment(class_tree_final_fit, new_data = df_test) %>%
  conf_mat(truth = type_1, estimate = .pred_class) %>%
  autoplot(type = "heatmap")

pred_result <- augment(class_tree_final_fit, new_data = df_test)
auc <- roc_auc(data = pred_result, truth = type_1, estimate = c(.pred_Bug, .pred_Fire, .pred_Grass, .pred_Normal, .pred_Water, .pred_Psychic ), estimator = "macro_weighted")
auc

```

